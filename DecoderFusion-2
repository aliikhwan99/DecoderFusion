{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12287144,"sourceType":"datasetVersion","datasetId":7743669},{"sourceId":12315409,"sourceType":"datasetVersion","datasetId":7762636}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Kaggle Environment Setup - Clean installation with isolated dependencies\n!pip install --force-reinstall numpy==1.26.4\n!pip install transformers==4.41.0 torch==2.1.2 sentencepiece phonemizer epitran spacy==3.7.4 sacrebleu importlib_metadata==7.1.0\n!python -m spacy download -q en_core_web_sm-3.7.0 --direct\n\n# Clean up potential conflicts\n!pip uninstall -y torchao accelerate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T14:37:06.427917Z","iopub.execute_input":"2025-12-28T14:37:06.428264Z","iopub.status.idle":"2025-12-28T14:37:25.253034Z","shell.execute_reply.started":"2025-12-28T14:37:06.428232Z","shell.execute_reply":"2025-12-28T14:37:25.252042Z"}},"outputs":[{"name":"stdout","text":"Collecting numpy==1.26.4\n  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\nUsing cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\nInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npeft 0.17.1 requires accelerate>=0.21.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\njaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\njax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\npytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.26.4\nCollecting transformers==4.41.0\n  Using cached transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.1.2 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.7.1, 2.8.0, 2.9.0, 2.9.1)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.1.2\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: -q-py3-none-any.whl is not a valid wheel filename.\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: Skipping torchao as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping accelerate as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install -q sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T14:37:25.254325Z","iopub.execute_input":"2025-12-28T14:37:25.254701Z","iopub.status.idle":"2025-12-28T14:37:29.114537Z","shell.execute_reply.started":"2025-12-28T14:37:25.254652Z","shell.execute_reply":"2025-12-28T14:37:29.113303Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"!pip install -q epitran panphon\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T14:37:29.120174Z","iopub.execute_input":"2025-12-28T14:37:29.120571Z","iopub.status.idle":"2025-12-28T14:37:32.982993Z","shell.execute_reply.started":"2025-12-28T14:37:29.120490Z","shell.execute_reply":"2025-12-28T14:37:32.981807Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nos.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n\nimport json, time, random\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\n\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer, get_scheduler\nfrom sacrebleu import corpus_bleu\nfrom tqdm import tqdm\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T14:43:14.388707Z","iopub.execute_input":"2025-12-28T14:43:14.389460Z","iopub.status.idle":"2025-12-28T14:43:42.163388Z","shell.execute_reply.started":"2025-12-28T14:43:14.389418Z","shell.execute_reply":"2025-12-28T14:43:42.162247Z"}},"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766933006.099224     168 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766933006.171058     168 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766933006.759724     168 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766933006.759802     168 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766933006.759807     168 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766933006.759809     168 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"class MorphologyEmbedder(nn.Module):\n    def __init__(self, device):\n        super().__init__()\n        from transformers import BertTokenizer, BertModel\n\n        self.device = device\n        self.tokenizer = BertTokenizer.from_pretrained(\"imvladikon/charbert-bert-wiki\")\n        self.model = BertModel.from_pretrained(\n            \"imvladikon/charbert-bert-wiki\"\n        ).to(device)\n\n        for p in self.model.parameters():\n            p.requires_grad = False\n\n        self.char_embedding = nn.Embedding(128, 64).to(device)\n        self.embedding_dim = self.model.config.hidden_size + 64\n\n    def forward(self, texts: List[str]):\n        inputs = self.tokenizer(\n            texts, padding=True, truncation=True,\n            max_length=128, return_tensors=\"pt\"\n        ).to(self.device)\n\n        with torch.no_grad():\n            bert_emb = self.model(**inputs).last_hidden_state.mean(dim=1)\n\n        char_embs = []\n        for t in texts:\n            chars = [ord(c) if ord(c) < 128 else 0 for c in t[:64]]\n            chars += [0] * (64 - len(chars))\n            emb = self.char_embedding(\n                torch.tensor(chars, device=self.device)\n            ).mean(dim=0)\n            char_embs.append(emb)\n\n        char_embs = torch.stack(char_embs)\n        return torch.cat([bert_emb, char_embs], dim=-1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T14:44:19.965387Z","iopub.execute_input":"2025-12-28T14:44:19.965881Z","iopub.status.idle":"2025-12-28T14:44:19.976810Z","shell.execute_reply.started":"2025-12-28T14:44:19.965845Z","shell.execute_reply":"2025-12-28T14:44:19.975762Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class KelantanesePhonemeEmbeddings(nn.Module):\n    def __init__(self, device, embedding_dim=256):\n        super().__init__()\n        self.device = device\n\n        self.phoneme_map = {p: i for i, p in enumerate(\"pbtdkgmnshlrwjiɛaəɔou\")}\n        self.pad_id = len(self.phoneme_map)\n\n        self.embedding = nn.Embedding(\n            len(self.phoneme_map) + 1, embedding_dim\n        ).to(device)\n\n        try:\n            import epitran\n            self.epi = epitran.Epitran(\"msa-Latn\")\n            self.use_epitran = True\n        except:\n            self.use_epitran = False\n\n    def forward(self, texts):\n        embs = []\n        for t in texts:\n            phonemes = (\n                [p for p in self.epi.transliterate(t) if p in self.phoneme_map]\n                if self.use_epitran else list(t)\n            )[:64]\n\n            ids = [self.phoneme_map.get(p, self.pad_id) for p in phonemes]\n            ids += [self.pad_id] * (64 - len(ids))\n\n            ids = torch.tensor(ids, device=self.device)\n            embs.append(self.embedding(ids).mean(dim=0))\n\n        return torch.stack(embs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T14:45:04.270826Z","iopub.execute_input":"2025-12-28T14:45:04.271902Z","iopub.status.idle":"2025-12-28T14:45:04.283661Z","shell.execute_reply.started":"2025-12-28T14:45:04.271851Z","shell.execute_reply":"2025-12-28T14:45:04.282572Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class KelantaneseDecoderOnly(nn.Module):\n    def __init__(self, model_name=\"distilgpt2\"):\n        super().__init__()\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.tokenizer.padding_side = \"left\"\n\n        self.model = GPT2LMHeadModel.from_pretrained(model_name).to(self.device)\n\n        self.morph = MorphologyEmbedder(self.device)\n        self.phoneme = KelantanesePhonemeEmbeddings(self.device)\n\n        self.fusion = nn.Sequential(\n            nn.Linear(\n                self.morph.embedding_dim + 256,\n                self.model.config.n_embd\n            ),\n            nn.GELU(),\n            nn.LayerNorm(self.model.config.n_embd)\n        ).to(self.device)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        texts = self.tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n\n        fused = self.fusion(\n            torch.cat([\n                self.morph(texts),\n                self.phoneme(texts)\n            ], dim=-1)\n        )\n\n        token_embs = self.model.transformer.wte(input_ids)\n        token_embs = token_embs + fused.unsqueeze(1)\n\n        return self.model(\n            inputs_embeds=token_embs,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T14:47:12.400416Z","iopub.execute_input":"2025-12-28T14:47:12.400925Z","iopub.status.idle":"2025-12-28T14:47:12.412332Z","shell.execute_reply.started":"2025-12-28T14:47:12.400882Z","shell.execute_reply":"2025-12-28T14:47:12.411309Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class DecoderTranslationDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=128):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        src, tgt = self.data[idx]\n\n        prompt = f\"Source: {src}\\nTarget:\"\n        full = f\"{prompt} {tgt}\"\n\n        enc = self.tokenizer(\n            full,\n            max_length=self.max_length,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n\n        input_ids = enc.input_ids.squeeze()\n        attention_mask = enc.attention_mask.squeeze()\n        labels = input_ids.clone()\n\n        prompt_len = len(\n            self.tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n        )\n\n        labels[:prompt_len] = -100  # ✅ FIX\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels,\n            \"prompt\": prompt,\n            \"target\": tgt\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T14:47:24.301107Z","iopub.execute_input":"2025-12-28T14:47:24.301555Z","iopub.status.idle":"2025-12-28T14:47:24.312314Z","shell.execute_reply.started":"2025-12-28T14:47:24.301514Z","shell.execute_reply":"2025-12-28T14:47:24.311242Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"jsonl_path = \"/kaggle/input/finaldialectdataset/finalDialect dataset.jsonl\"\n\nall_data = []\nwith open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        obj = json.loads(line)\n        src = obj.get(\"kelantanese\")\n        tgt = obj.get(\"stdMalay\") or obj.get(\"english\")\n        if src and tgt:\n            all_data.append((src, tgt))\n\nrandom.shuffle(all_data)\nsplit = int(0.8 * len(all_data))\ntrain_data = all_data[:split]\ntest_data  = all_data[split:]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T14:47:38.454693Z","iopub.execute_input":"2025-12-28T14:47:38.455163Z","iopub.status.idle":"2025-12-28T14:47:38.486132Z","shell.execute_reply.started":"2025-12-28T14:47:38.455122Z","shell.execute_reply":"2025-12-28T14:47:38.485182Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"model = KelantaneseDecoderOnly()\ntokenizer = model.tokenizer\ndevice = model.device\n\ntrain_loader = DataLoader(\n    DecoderTranslationDataset(train_data, tokenizer),\n    batch_size=4,\n    shuffle=True\n)\n\ntest_loader = DataLoader(\n    DecoderTranslationDataset(test_data, tokenizer),\n    batch_size=4\n)\n\noptimizer = AdamW(model.parameters(), lr=3e-4)\nepochs = 3\nsteps = len(train_loader) * epochs\n\nscheduler = get_scheduler(\n    \"linear\",\n    optimizer,\n    num_warmup_steps=int(0.1 * steps),\n    num_training_steps=steps\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T14:47:50.858229Z","iopub.execute_input":"2025-12-28T14:47:50.858601Z","iopub.status.idle":"2025-12-28T14:48:12.787999Z","shell.execute_reply.started":"2025-12-28T14:47:50.858570Z","shell.execute_reply":"2025-12-28T14:48:12.786767Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f34a0fae1c2406894391c7bd9978f26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"534dafb99d1f44e0822cbc2030602ab9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69e68d21d5aa49fe882e57c1f2f4b828"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"350ed6e23f704af8bd0391a7d353c792"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9405fbb7abc940e49253dd04025900e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b208ac864fcb4713a65cbf514d7dde9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ec93cabfd7449d08b91a61d7c00eb2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b80b776cd83e4c0898bb851fa4dd3f8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/682 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"545d21500f31484ca8c98213d22e48ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/552M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2674e29de33d48b58b7d3ed4f5007c67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/552M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf4f8213e17148a9986cf36fc943c614"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"for epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    start = time.time()\n\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        batch = {k: v.to(device) for k, v in batch.items() if k not in [\"prompt\", \"target\"]}\n        loss = model(**batch).loss\n\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_loader)\n\n    # -------- BLEU --------\n    model.eval()\n    preds, refs = [], []\n\n    with torch.no_grad():\n        for batch in test_loader:\n            inputs = tokenizer(\n                batch[\"prompt\"],\n                return_tensors=\"pt\",\n                padding=True\n            ).to(device)\n\n            gen = model.model.generate(\n                **inputs,\n                max_new_tokens=40,\n                do_sample=False,\n                pad_token_id=tokenizer.eos_token_id\n            )\n\n            decoded = tokenizer.batch_decode(gen, skip_special_tokens=True)\n\n            for d, tgt in zip(decoded, batch[\"target\"]):\n                if \"Target:\" in d:\n                    preds.append(d.split(\"Target:\")[-1].strip())\n                    refs.append([tgt])\n\n    bleu = corpus_bleu(preds, refs).score\n    print(\n        f\"Epoch {epoch+1} | Loss {avg_loss:.4f} | BLEU {bleu:.2f} | Time {time.time()-start:.1f}s\"\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T14:49:34.106836Z","iopub.execute_input":"2025-12-28T14:49:34.108048Z"}},"outputs":[{"name":"stderr","text":"Epoch 1:   0%|          | 0/377 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\nEpoch 1:  64%|██████▍   | 241/377 [14:12<08:00,  3.53s/it]","output_type":"stream"}],"execution_count":null}]}