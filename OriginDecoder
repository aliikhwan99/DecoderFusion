{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12287144,"sourceType":"datasetVersion","datasetId":7743669},{"sourceId":12315409,"sourceType":"datasetVersion","datasetId":7762636}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Kaggle Environment Setup - Clean installation with isolated dependencies\n!pip install --force-reinstall numpy==1.26.4\n!pip install transformers==4.41.0 torch==2.1.2 sentencepiece phonemizer epitran spacy==3.7.4 sacrebleu importlib_metadata==7.1.0\n!python -m spacy download -q en_core_web_sm-3.7.0 --direct\n\n# Clean up potential conflicts\n!pip uninstall -y torchao accelerate\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q sacrebleu","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q epitran panphon\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GPT-ONLY","metadata":{}},{"cell_type":"code","source":"# =========================\n# C1: Environment\n# =========================\nimport os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n\n# =========================\n# C2: Imports\n# =========================\nimport json\nimport time\nimport random\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\n\nfrom transformers import (\n    GPT2LMHeadModel,\n    GPT2Tokenizer,\n    get_scheduler\n)\n\nfrom sacrebleu import corpus_bleu\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\n# =========================\n# C3: Decoder-only Dataset\n# =========================\nclass DecoderTranslationDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=128):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        src, tgt = self.data[idx]\n\n        prompt = f\"Source: {src}\\nTarget:\"\n        full_text = f\"{prompt} {tgt}\"\n\n        enc = self.tokenizer(\n            full_text,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n\n        input_ids = enc.input_ids.squeeze()\n        attention_mask = enc.attention_mask.squeeze()\n        labels = input_ids.clone()\n\n        # ğŸ”‘ mask source tokens\n        prompt_len = len(\n            self.tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n        )\n        labels[:prompt_len] = -100\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels,\n            \"target\": tgt\n        }\n\n# =========================\n# C4: Pure Decoder-only GPT\n# =========================\nclass KelantaneseDecoderOnly(nn.Module):\n    def __init__(self, model_name=\"distilgpt2\"):\n        super().__init__()\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.tokenizer.padding_side = \"left\"  # REQUIRED\n\n        self.model = GPT2LMHeadModel.from_pretrained(model_name).to(self.device)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        return self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n\n# =========================\n# C5: Load Dataset\n# =========================\njsonl_path = \"/kaggle/input/finaldialectdataset/finalDialect dataset.jsonl\"\n\nall_data = []\nwith open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        obj = json.loads(line.strip())\n        src = obj.get(\"kelantanese\")\n        tgt = obj.get(\"stdMalay\") or obj.get(\"english\")\n        if src and tgt:\n            all_data.append((src, tgt))\n\nrandom.shuffle(all_data)\n\n# ğŸ”½ limit size for speed (optional)\ntrain_data = all_data[:512]\ntest_data  = all_data[512:640]\n\nprint(f\"Train: {len(train_data)} | Test: {len(test_data)}\")\n\n# =========================\n# C6: Dataloader\n# =========================\nmodel = KelantaneseDecoderOnly()\ntokenizer = model.tokenizer\ndevice = model.device\n\ntrain_loader = DataLoader(\n    DecoderTranslationDataset(train_data, tokenizer),\n    batch_size=4,\n    shuffle=True\n)\n\ntest_loader = DataLoader(\n    DecoderTranslationDataset(test_data, tokenizer),\n    batch_size=8\n)\n\n# =========================\n# C7: Optimizer & Scheduler\n# =========================\noptimizer = AdamW(model.parameters(), lr=5e-4)\nepochs = 5\nsteps = len(train_loader) * epochs\n\nscheduler = get_scheduler(\n    \"linear\",\n    optimizer,\n    num_warmup_steps=int(0.1 * steps),\n    num_training_steps=steps\n)\n\n# =========================\n# C8: Training + BLEU\n# =========================\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    start = time.time()\n\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        batch = {k: v.to(device) for k, v in batch.items() if k != \"target\"}\n\n        out = model(**batch)\n        loss = out.loss\n\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_loader)\n\n    # ---------- BLEU ----------\n    model.eval()\n    preds, refs = [], []\n\n    with torch.no_grad():\n        for batch in test_loader:\n            inputs = batch[\"input_ids\"].to(device)\n            masks  = batch[\"attention_mask\"].to(device)\n\n            gen = model.model.generate(\n                input_ids=inputs,\n                attention_mask=masks,\n                max_new_tokens=40,\n                pad_token_id=tokenizer.eos_token_id\n            )\n\n            decoded = tokenizer.batch_decode(gen, skip_special_tokens=True)\n\n            for d, tgt in zip(decoded, batch[\"target\"]):\n                if \"Target:\" in d:\n                    preds.append(d.split(\"Target:\")[-1].strip())\n                    refs.append([tgt])\n\n    bleu = corpus_bleu(preds, refs).score if preds else 0.0\n\n    print(\n        f\"Epoch {epoch+1} | \"\n        f\"Loss {avg_loss:.4f} | \"\n        f\"BLEU {bleu:.2f} | \"\n        f\"Time {time.time()-start:.1f}s\"\n    )\n\n# =========================\n# C9: Sample Predictions\n# =========================\nmodel.eval()\nfor src, tgt in random.sample(test_data, min(5, len(test_data))):\n    prompt = f\"Source: {src}\\nTarget:\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n    gen = model.model.generate(\n        **inputs,\n        max_new_tokens=40,\n        pad_token_id=tokenizer.eos_token_id\n    )\n\n    out = tokenizer.decode(gen[0], skip_special_tokens=True)\n\n    print(\"SOURCE:\", src)\n    print(\"PRED  :\", out.split(\"Target:\")[-1].strip())\n    print(\"GOLD  :\", tgt)\n    print(\"-\" * 60)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ---VERSION 2 DECODER ONLY","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade accelerate\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Restart Kaggle kernel first\n!pip uninstall -y transformers peft\n!pip install transformers==4.37.0  # stable version\n!pip install datasets sacrebleu torch\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q peft==0.7.1 accelerate==0.25.0\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q \\\n  transformers==4.37.0 \\\n  accelerate==0.26.1 \\\n  peft==0.7.1 \\\n  datasets sacrebleu\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q transformers==4.37.0 accelerate==0.25.0 peft==0.7.1 datasets sacrebleu\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# 0ï¸âƒ£ Environment\n# =========================================================\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# =========================================================\n# 1ï¸âƒ£ Imports\n# =========================================================\nimport json\nimport ast\nimport random\nimport torch\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    Trainer,\n    TrainingArguments\n)\nfrom sacrebleu.metrics import CHRF\n\n# =========================================================\n# 2ï¸âƒ£ Paths (FOLDERS, not files)\n# =========================================================\nMANGLISH_DIR = \"/kaggle/input/filtered-manglish-1-8kv2\"\nKELANTAN_DIR = \"/kaggle/input/finaldialectdataset\"\n\n# =========================================================\n# 3ï¸âƒ£ Utility: list files safely\n# =========================================================\ndef list_files(folder):\n    files = []\n    for f in os.listdir(folder):\n        path = os.path.join(folder, f)\n        if os.path.isfile(path):\n            files.append(path)\n    return files\n\nmanglish_files = list_files(MANGLISH_DIR)\nkelantan_files = list_files(KELANTAN_DIR)\n\nprint(\"Manglish files:\", manglish_files)\nprint(\"Kelantan files:\", kelantan_files)\n\n# =========================================================\n# 4ï¸âƒ£ Load Kelantanese (proper JSONL)\n# =========================================================\ndef load_kelantanese(files):\n    data = []\n    for path in files:\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                try:\n                    row = json.loads(line)\n                except:\n                    continue\n                if \"kelantanese\" in row and \"stdMalay\" in row:\n                    data.append({\n                        \"dialect\": \"kelantanese\",\n                        \"source\": row[\"kelantanese\"].strip(),\n                        \"target\": row[\"stdMalay\"].strip()\n                    })\n    return data\n\n# =========================================================\n# 5ï¸âƒ£ Load Manglish (LIST-style, broken JSON)\n# =========================================================\ndef load_manglish(files):\n    data = []\n    for path in files:\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            for line_num, line in enumerate(f, 1):\n                line = line.strip()\n                if not line:\n                    continue\n\n                try:\n                    row = json.loads(line)\n                except:\n                    try:\n                        row = ast.literal_eval(line)\n                    except:\n                        continue\n\n                # Expected:\n                # [\"manglish\", \"text\", {\"malay\": \"...\"}]\n                if (\n                    isinstance(row, list)\n                    and len(row) >= 3\n                    and isinstance(row[2], dict)\n                    and \"malay\" in row[2]\n                ):\n                    data.append({\n                        \"dialect\": \"manglish\",\n                        \"source\": str(row[1]).strip(),\n                        \"target\": row[2][\"malay\"].strip()\n                    })\n    return data\n\n# =========================================================\n# 6ï¸âƒ£ Load + Shuffle Data\n# =========================================================\ndata = load_kelantanese(kelantan_files) + load_manglish(manglish_files)\nrandom.shuffle(data)\n\nprint(f\"âœ… Total samples loaded: {len(data)}\")\n\nassert len(data) > 0, \"âŒ Dataset is EMPTY â€” check file paths!\"\n\n# =========================================================\n# 7ï¸âƒ£ Train / Test Split\n# =========================================================\nsplit = int(0.85 * len(data))\ntrain_data = data[:split]\ntest_data = data[split:]\n\ntrain_ds = Dataset.from_list(train_data)\ntest_ds = Dataset.from_list(test_data)\n\nprint(f\"Train: {len(train_ds)} | Test: {len(test_ds)}\")\n\n# =========================================================\n# 8ï¸âƒ£ GPT-2 Model + Tokenizer\n# =========================================================\nMODEL_NAME = \"gpt2\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n\nMAX_LEN = 256\n\n# =========================================================\n# 9ï¸âƒ£ Preprocessing (Causal LM, masked prompt)\n# =========================================================\ndef preprocess(batch):\n    input_ids, labels, attention_mask = [], [], []\n\n    for d, s, t in zip(batch[\"dialect\"], batch[\"source\"], batch[\"target\"]):\n        prompt = (\n            f\"<|dialect|> {d}\\n\"\n            f\"<|source|> {s}\\n\"\n            f\"<|target|> \"\n        )\n\n        full = prompt + t + tokenizer.eos_token\n\n        enc = tokenizer(\n            full,\n            max_length=MAX_LEN,\n            truncation=True,\n            padding=\"max_length\"\n        )\n\n        label = enc[\"input_ids\"].copy()\n        prompt_len = len(tokenizer(prompt)[\"input_ids\"])\n\n        for i in range(prompt_len):\n            label[i] = -100\n\n        input_ids.append(enc[\"input_ids\"])\n        labels.append(label)\n        attention_mask.append(enc[\"attention_mask\"])\n\n    return {\n        \"input_ids\": input_ids,\n        \"labels\": labels,\n        \"attention_mask\": attention_mask\n    }\n\ntrain_ds = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\ntest_ds = test_ds.map(preprocess, batched=True, remove_columns=test_ds.column_names)\n\ntrain_ds.set_format(\"torch\")\ntest_ds.set_format(\"torch\")\n\n# =========================================================\n# ğŸ”Ÿ Metrics (chrF + exact)\n# =========================================================\nchrf = CHRF()\n\ndef compute_metrics(eval_pred):\n    preds, labels = eval_pred\n    preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    labels = tokenizer.batch_decode(\n        [[l for l in lab if l != -100] for lab in labels],\n        skip_special_tokens=True\n    )\n\n    preds = [p.split(\"<|target|>\")[-1].strip() for p in preds]\n\n    return {\n        \"chrF\": chrf.corpus_score(preds, [labels]).score,\n        \"exact_match\": sum(p == l for p, l in zip(preds, labels)) / len(preds)\n    }\n\n# =========================================================\n# 1ï¸âƒ£1ï¸âƒ£ Training Args (SMALL DATA SAFE)\n# =========================================================\nargs = TrainingArguments(\n    output_dir=\"./gpt_dialect\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=3e-5,\n    num_train_epochs=10,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=8,\n    fp16=torch.cuda.is_available(),\n    logging_steps=50,\n    save_total_limit=2,\n    report_to=\"none\"\n)\n\n# =========================================================\n# 1ï¸âƒ£2ï¸âƒ£ Trainer\n# =========================================================\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_ds,\n    eval_dataset=test_ds,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n\n# =========================================================\n# 1ï¸âƒ£3ï¸âƒ£ Inference\n# =========================================================\ndef translate(text, dialect):\n    prompt = (\n        f\"<|dialect|> {dialect}\\n\"\n        f\"<|source|> {text}\\n\"\n        f\"<|target|> \"\n    )\n    ids = tokenize\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# -----------","metadata":{}},{"cell_type":"markdown","source":"# Shorter Step Version","metadata":{}},{"cell_type":"code","source":"# =========================================================\n# ğŸ”¥ GPT2 DIALECT â†’ MALAY (MANUAL BLEU EVALUATION)\n# Kaggle | ONE CELL | 50 STEPS | STABLE\n# =========================================================\n\n# ---------- 0ï¸âƒ£ INSTALL (SAFE SET) ----------\n!pip uninstall -y transformers accelerate peft -q\n!pip install transformers==4.37.0 accelerate==0.27.2 datasets sacrebleu -q\n\nimport os, json, random, torch\nfrom glob import glob\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\nfrom sacrebleu.metrics import BLEU\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# ---------- 1ï¸âƒ£ PATHS ----------\nMANGLISH_DIR = \"/kaggle/input/filtered-manglish-1-8kv2\"\nKELANTAN_DIR = \"/kaggle/input/finaldialectdataset\"\n\nmanglish_files = glob(f\"{MANGLISH_DIR}/*.jsonl\")\nkelantan_files = glob(f\"{KELANTAN_DIR}/*.jsonl\")\n\nprint(\"Manglish files:\", manglish_files)\nprint(\"Kelantan files:\", kelantan_files)\n\n# ---------- 2ï¸âƒ£ SAFE LOADERS ----------\ndef safe_jsonl(path):\n    rows = []\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            try:\n                rows.append(json.loads(line.strip()))\n            except:\n                continue\n    return rows\n\ndef load_kelantan(files):\n    out = []\n    for f in files:\n        for r in safe_jsonl(f):\n            if \"kelantanese\" in r and \"stdMalay\" in r:\n                out.append({\n                    \"dialect\": \"kelantanese\",\n                    \"source\": r[\"kelantanese\"],\n                    \"target\": r[\"stdMalay\"]\n                })\n    return out\n\ndef load_manglish(files):\n    out = []\n    for f in files:\n        for r in safe_jsonl(f):\n            if \"text\" in r and \"malay\" in r:\n                out.append({\n                    \"dialect\": \"manglish\",\n                    \"source\": r[\"text\"],\n                    \"target\": r[\"malay\"]\n                })\n    return out\n\ndata = load_kelantan(kelantan_files) + load_manglish(manglish_files)\nrandom.shuffle(data)\n\nprint(f\"âœ… Total samples loaded: {len(data)}\")\n\n# ---------- 3ï¸âƒ£ SPLIT ----------\nsplit = int(0.85 * len(data))\ntrain_data = data[:split]\neval_data  = data[split:]\n\ntrain_ds = Dataset.from_list(train_data)\neval_ds  = Dataset.from_list(eval_data)\n\nprint(f\"Train: {len(train_ds)} | Eval: {len(eval_ds)}\")\n\n# ---------- 4ï¸âƒ£ MODEL ----------\nMODEL = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(MODEL)\n\nMAX_LEN = 256\n\n# ---------- 5ï¸âƒ£ TOKENIZE ----------\ndef preprocess(batch):\n    input_ids, labels, masks = [], [], []\n\n    for d, s, t in zip(batch[\"dialect\"], batch[\"source\"], batch[\"target\"]):\n        prompt = f\"<|dialect|> {d}\\n<|source|> {s}\\n<|target|> \"\n        full = prompt + t + tokenizer.eos_token\n\n        enc = tokenizer(full, truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n        lab = enc[\"input_ids\"].copy()\n\n        p_len = len(tokenizer(prompt)[\"input_ids\"])\n        lab[:p_len] = [-100] * p_len\n\n        input_ids.append(enc[\"input_ids\"])\n        labels.append(lab)\n        masks.append(enc[\"attention_mask\"])\n\n    return {\n        \"input_ids\": input_ids,\n        \"labels\": labels,\n        \"attention_mask\": masks\n    }\n\ntrain_ds = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\neval_ds  = eval_ds.map(preprocess, batched=True, remove_columns=eval_ds.column_names)\n\ntrain_ds.set_format(\"torch\")\neval_ds.set_format(\"torch\")\n\n# ---------- 6ï¸âƒ£ TRAIN ----------\nargs = TrainingArguments(\n    output_dir=\"./gpt2-dialect\",\n    max_steps=50,                  # ğŸ”¥ ONLY 50 STEPS\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    learning_rate=5e-5,\n    logging_steps=5,\n    save_steps=50,\n    evaluation_strategy=\"no\",      # âš  GPT Trainer = loss only\n    report_to=\"none\",\n    fp16=torch.cuda.is_available()\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_ds,\n    tokenizer=tokenizer\n)\n\ntrainer.train()\n\n# =========================================================\n# 7ï¸âƒ£ MANUAL BLEU EVALUATION (CORRECT WAY FOR GPT)\n# =========================================================\n\nbleu = BLEU()\n\ndef generate(text, dialect):\n    prompt = f\"<|dialect|> {dialect}\\n<|source|> {text}\\n<|target|> \"\n    ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n    out = model.generate(ids, max_new_tokens=60)\n    return tokenizer.decode(out[0], skip_special_tokens=True).split(\"<|target|>\")[-1].strip()\n\npreds, refs = [], []\n\nfor ex in eval_data[:100]:   # ğŸ”¥ evaluate first 100 samples (fast)\n    pred = generate(ex[\"source\"], ex[\"dialect\"])\n    preds.append(pred)\n    refs.append(ex[\"target\"])\n\nbleu_score = bleu.corpus_score(preds, [refs]).score\n\nprint(\"\\nâœ… BLEU SCORE:\")\nprint(f\"BLEU = {bleu_score:.2f}\")\n\n# ---------- 8ï¸âƒ£ DEMO ----------\nprint(\"\\nKelantanese â†’ Malay:\")\nprint(generate(\"Nasi ni sedho sikit\", \"kelantanese\"))\n\nprint(\"\\nManglish â†’ Malay:\")\nprint(generate(\"aiyo dont do like that lah\", \"manglish\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FullRun","metadata":{}},{"cell_type":"code","source":"# =========================================================\n# ğŸ”¥ GPT-2 DECODER-ONLY (FAIR BASELINE)\n# Matched EXACTLY to Fusion Training Setup\n# =========================================================\n\n# ---------- 0ï¸âƒ£ ENVIRONMENT ----------\n!pip uninstall -y peft accelerate transformers -q\n!pip install -q transformers==4.37.0 accelerate==0.26.1 datasets sacrebleu torch\n\nimport os, json, glob, random, torch\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    Trainer,\n    TrainingArguments\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# =========================================================\n# 1ï¸âƒ£ LOAD DATA (IDENTICAL)\n# =========================================================\ndef load_jsonl_from_dir(directory, dialect, src_key, tgt_key):\n    data = []\n    for path in glob.glob(f\"{directory}/*.jsonl\"):\n        with open(path, encoding=\"utf-8\", errors=\"ignore\") as f:\n            for line in f:\n                try:\n                    r = json.loads(line)\n                    if src_key in r and tgt_key in r:\n                        data.append({\n                            \"dialect\": dialect,\n                            \"source\": r[src_key],\n                            \"target\": r[tgt_key]\n                        })\n                except:\n                    pass\n    return data\n\nMANGLISH_DIR = \"/kaggle/input/filtered-manglish-1-8kv2\"\nKELANTAN_DIR = \"/kaggle/input/finaldialectdataset\"\n\ndata = (\n    load_jsonl_from_dir(MANGLISH_DIR, \"manglish\", \"text\", \"malay\") +\n    load_jsonl_from_dir(KELANTAN_DIR, \"kelantanese\", \"kelantanese\", \"stdMalay\")\n)\n\nrandom.shuffle(data)\nsplit = int(0.85 * len(data))\ntrain_raw = data[:split]\neval_raw  = data[split:]\n\ntrain_ds = Dataset.from_list(train_raw)\neval_ds  = Dataset.from_list(eval_raw)\n\n# =========================================================\n# 2ï¸âƒ£ TOKENIZER (MATCHED)\n# =========================================================\nMODEL_NAME = \"gpt2\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\ntokenizer.add_special_tokens({\n    \"additional_special_tokens\": [\n        \"<|dialect|>\",\n        \"<|source|>\",\n        \"<|target|>\"\n    ]\n})\n\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"left\"   # ğŸ”¥ MATCH FUSION\n\nMAX_LEN = 256\n\n# =========================================================\n# 3ï¸âƒ£ MODEL (DECODER-ONLY)\n# =========================================================\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# =========================================================\n# 4ï¸âƒ£ PREPROCESSING (IDENTICAL PROMPT & MASKING)\n# =========================================================\ndef preprocess(batch):\n    ids, masks, labels = [], [], []\n\n    for d, s, t in zip(batch[\"dialect\"], batch[\"source\"], batch[\"target\"]):\n        prompt = f\"<|dialect|> {d}\\n<|source|> {s}\\n<|target|> \"\n        full = prompt + t + tokenizer.eos_token\n\n        enc = tokenizer(\n            full,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=MAX_LEN\n        )\n\n        lab = enc[\"input_ids\"].copy()\n        p_len = len(tokenizer(prompt)[\"input_ids\"])\n        lab[:p_len] = [-100] * p_len   # ğŸ”¥ target-only loss\n\n        ids.append(enc[\"input_ids\"])\n        masks.append(enc[\"attention_mask\"])\n        labels.append(lab)\n\n    return {\n        \"input_ids\": ids,\n        \"attention_mask\": masks,\n        \"labels\": labels\n    }\n\ntrain_ds = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\neval_ds  = eval_ds.map(preprocess, batched=True, remove_columns=eval_ds.column_names)\n\ntrain_ds.set_format(\"torch\")\neval_ds.set_format(\"torch\")\n\n# =========================================================\n# 5ï¸âƒ£ TRAINING (MATCHED 1-TO-1)\n# =========================================================\nargs = TrainingArguments(\n    output_dir=\"./decoder-only\",\n\n    # ===== BATCHING =====\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=8,   # ğŸ”¥ effective batch = 16\n\n    # ===== OPTIMIZATION =====\n    learning_rate=2e-5,              # ğŸ”¥ SAME AS FUSION\n    warmup_steps=800,\n    max_steps=4000,\n\n    lr_scheduler_type=\"cosine\",\n    weight_decay=0.01,\n    max_grad_norm=1.0,\n\n    # ===== LOGGING =====\n    logging_steps=100,\n\n    # ===== SYSTEM =====\n    save_strategy=\"no\",\n    evaluation_strategy=\"no\",\n    report_to=\"none\",\n    fp16=torch.cuda.is_available(),\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_ds,\n    tokenizer=tokenizer\n)\n\ntrainer.train()\n\n# =========================================================\n# 6ï¸âƒ£ BLEU EVALUATION (SAME METHOD)\n# =========================================================\nfrom sacrebleu.metrics import BLEU\nbleu = BLEU()\n\ndef evaluate_bleu(model, raw_data, n=100):\n    model.eval()\n    preds, refs = [], []\n\n    for ex in raw_data[:n]:\n        prompt = f\"<|dialect|> {ex['dialect']}\\n<|source|> {ex['source']}\\n<|target|> \"\n        enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n        with torch.no_grad():\n            out = model.generate(\n                **enc,\n                max_new_tokens=50,\n                num_beams=4,\n                pad_token_id=tokenizer.eos_token_id\n            )\n\n        pred = tokenizer.decode(out[0], skip_special_tokens=True)\n        pred = pred.split(\"<|target|>\")[-1].strip()\n\n        preds.append(pred)\n        refs.append(ex[\"target\"])\n\n    return bleu.corpus_score(preds, [refs]).score\n\nprint(\"Decoder-only BLEU:\", evaluate_bleu(model, eval_raw))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T14:27:38.171846Z","iopub.execute_input":"2026-01-02T14:27:38.172358Z","iopub.status.idle":"2026-01-02T16:54:48.523734Z","shell.execute_reply.started":"2026-01-02T14:27:38.172329Z","shell.execute_reply":"2026-01-02T16:54:48.522877Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.37.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2026-01-02 14:29:18.767894: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767364158.988226      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767364159.055275      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e08cbe5723c4f52bb7d08db8cf5679e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"257e4f897ea44e20854b570a0610c9ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ae15f1c5d164cada07e456704c61757"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c91da07300b9494199ddaaa0573f01b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b67b787221bb4bfeb425da381ab9cc23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0356d6822ec5485db167caa730127f67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"091c087ef25e43548231f41e09e542ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1603 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82dd4a9a95d1471986d3e17f481adf66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/283 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ed5350d19074349bfbe0ca5dd4775b7"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4000/4000 2:24:35, Epoch 79/80]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>15.361000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.992400</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.649600</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.529200</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.452700</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.441100</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.441200</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>2.043900</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>6.705000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.994600</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.357300</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.314400</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.289300</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.262300</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.250900</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.235500</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.231300</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.227400</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.221900</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.215500</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.212600</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.205800</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.204100</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.200200</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.199100</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.195900</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.193400</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.189200</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.189200</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.187900</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.183500</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.184700</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.182200</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.183200</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.181700</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.181300</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.180300</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.180700</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.180400</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.180200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Decoder-only BLEU: 5.280700194433655\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"def show_test_predictions(model, raw_data, n=10):\n    model.eval()\n\n    for i, ex in enumerate(raw_data[:n]):\n        prompt = f\"<|dialect|> {ex['dialect']}\\n<|source|> {ex['source']}\\n<|target|> \"\n        enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n        with torch.no_grad():\n            out = model.generate(\n                **enc,\n                max_new_tokens=50,\n                num_beams=4,\n                pad_token_id=tokenizer.eos_token_id\n            )\n\n        pred = tokenizer.decode(out[0], skip_special_tokens=True)\n        pred = pred.split(\"<|target|>\")[-1].strip()\n\n        print(\"=\" * 60)\n        print(f\"[{i+1}] DIALECT : {ex['dialect']}\")\n        print(f\"SOURCE    : {ex['source']}\")\n        print(f\"REFERENCE : {ex['target']}\")\n        print(f\"PREDICTED : {pred}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T17:39:30.558115Z","iopub.execute_input":"2026-01-02T17:39:30.558784Z","iopub.status.idle":"2026-01-02T17:39:30.564608Z","shell.execute_reply.started":"2026-01-02T17:39:30.558751Z","shell.execute_reply":"2026-01-02T17:39:30.564048Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"show_test_predictions(model, eval_raw, n=10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T17:39:39.488030Z","iopub.execute_input":"2026-01-02T17:39:39.488733Z","iopub.status.idle":"2026-01-02T17:39:41.100878Z","shell.execute_reply.started":"2026-01-02T17:39:39.488707Z","shell.execute_reply":"2026-01-02T17:39:41.100273Z"}},"outputs":[{"name":"stdout","text":"============================================================\n[1] DIALECT : kelantanese\nSOURCE    : Esok kita nak kena ngejah bab perniagaan.\nREFERENCE : Esok kita perlu berunding tentang perniagaan.\nPREDICTED : kelantanese\n Esok kita nak kena ngejah bab perniagaan.\n============================================================\n[2] DIALECT : kelantanese\nSOURCE    : patat siput\nREFERENCE : kemahiran\nPREDICTED : kelantanese\n patat siput\n============================================================\n[3] DIALECT : kelantanese\nSOURCE    : Pak Long tu jjughuh orangnya, tok pernah marah.\nREFERENCE : Pak Long itu baik orangnya, tak pernah marah.\nPREDICTED : kelantanese\n Pak Long tu jjughuh orangnya, tok pernah marah.\n============================================================\n[4] DIALECT : kelantanese\nSOURCE    : abe\nREFERENCE : abang\nPREDICTED : kelantanese\n abe\n============================================================\n[5] DIALECT : kelantanese\nSOURCE    : colek\nREFERENCE : pencicah\nPREDICTED : kelantanese\n colek\n auauauauauauauauauauauauauauauauauau\n============================================================\n[6] DIALECT : kelantanese\nSOURCE    : risau\nREFERENCE : risau\nPREDICTED : kelantanese\n risau\n au\n============================================================\n[7] DIALECT : kelantanese\nSOURCE    : juruh\nREFERENCE : baik\nPREDICTED : kelantanese\n juruh\n============================================================\n[8] DIALECT : kelantanese\nSOURCE    : dale duo lagi\nREFERENCE : tidak pasti lagi\nPREDICTED : kelantanese\n dale duo lagi\n============================================================\n[9] DIALECT : kelantanese\nSOURCE    : khenak\nREFERENCE : membuat jahat\nPREDICTED : kelantanese\n khenak\n============================================================\n[10] DIALECT : kelantanese\nSOURCE    : Budok tu ngakok naik tangga, comel tengok.\nREFERENCE : Budak itu merangkak naik tangga, sangat comel dilihat.\nPREDICTED : kelantanese\n Budok tu ngakok naik tangga, comel tengok.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\n\ndef save_test_predictions(model, raw_data, path=\"decoder_only_predictions.csv\"):\n    rows = []\n    model.eval()\n\n    for ex in raw_data:\n        prompt = f\"<|dialect|> {ex['dialect']}\\n<|source|> {ex['source']}\\n<|target|> \"\n        enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n        with torch.no_grad():\n            out = model.generate(\n                **enc,\n                max_new_tokens=50,\n                num_beams=4,\n                pad_token_id=tokenizer.eos_token_id\n            )\n\n        pred = tokenizer.decode(out[0], skip_special_tokens=True)\n        pred = pred.split(\"<|target|>\")[-1].strip()\n\n        rows.append({\n            \"dialect\": ex[\"dialect\"],\n            \"source\": ex[\"source\"],\n            \"reference\": ex[\"target\"],\n            \"prediction\": pred\n        })\n\n    df = pd.DataFrame(rows)\n    df.to_csv(path, index=False)\n    print(f\"Saved predictions to {path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T17:40:50.030375Z","iopub.execute_input":"2026-01-02T17:40:50.030780Z","iopub.status.idle":"2026-01-02T17:40:50.036670Z","shell.execute_reply.started":"2026-01-02T17:40:50.030755Z","shell.execute_reply":"2026-01-02T17:40:50.036036Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"!ls /kaggle/working\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T17:44:53.942942Z","iopub.execute_input":"2026-01-02T17:44:53.943536Z","iopub.status.idle":"2026-01-02T17:44:54.128414Z","shell.execute_reply.started":"2026-01-02T17:44:53.943512Z","shell.execute_reply":"2026-01-02T17:44:54.127499Z"}},"outputs":[{"name":"stdout","text":"decoder-only\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"TrainingArguments(output_dir=\"./decoder-only\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T17:46:55.563150Z","iopub.execute_input":"2026-01-02T17:46:55.563451Z","iopub.status.idle":"2026-01-02T17:46:55.572253Z","shell.execute_reply.started":"2026-01-02T17:46:55.563429Z","shell.execute_reply":"2026-01-02T17:46:55.571610Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"TrainingArguments(\n_n_gpu=2,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=False,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_steps=None,\nevaluation_strategy=IntervalStrategy.NO,\nfp16=False,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=HubStrategy.EVERY_SAVE,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=5e-05,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=./decoder-only/runs/Jan02_17-46-55_f958d2bf6e61,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=500,\nlogging_strategy=IntervalStrategy.STEPS,\nlr_scheduler_kwargs={},\nlr_scheduler_type=SchedulerType.LINEAR,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=OptimizerNames.ADAMW_TORCH,\noptim_args=None,\noutput_dir=./decoder-only,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=8,\nper_device_train_batch_size=8,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard', 'wandb'],\nresume_from_checkpoint=None,\nrun_name=./decoder-only,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=True,\nsave_steps=500,\nsave_strategy=IntervalStrategy.STEPS,\nsave_total_limit=None,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=False,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=0,\nweight_decay=0.0,\n)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"save_test_predictions(\n    model,\n    eval_raw,\n    path=\"/kaggle/working/decoder_only_predictions.csv\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T17:47:46.851833Z","iopub.execute_input":"2026-01-02T17:47:46.852672Z","iopub.status.idle":"2026-01-02T17:49:12.243448Z","shell.execute_reply.started":"2026-01-02T17:47:46.852624Z","shell.execute_reply":"2026-01-02T17:49:12.242848Z"}},"outputs":[{"name":"stdout","text":"Saved predictions to /kaggle/working/decoder_only_predictions.csv\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!ls /kaggle/working\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T17:52:23.566318Z","iopub.execute_input":"2026-01-02T17:52:23.566909Z","iopub.status.idle":"2026-01-02T17:52:23.741965Z","shell.execute_reply.started":"2026-01-02T17:52:23.566881Z","shell.execute_reply":"2026-01-02T17:52:23.741028Z"}},"outputs":[{"name":"stdout","text":"decoder-only  decoder_only_predictions.csv\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Separate by dialect\nkelantanese = [x for x in data if x[\"dialect\"] == \"kelantanese\"]\nmanglish    = [x for x in data if x[\"dialect\"] == \"manglish\"]\n\n# Shuffle separately\nrandom.shuffle(kelantanese)\nrandom.shuffle(manglish)\n\n# Split per dialect\ndef split_data(lst, ratio=0.85):\n    n = int(len(lst) * ratio)\n    return lst[:n], lst[n:]\n\nkel_train, kel_test = split_data(kelantanese)\nman_train, man_test = split_data(manglish)\n\n# Combine\ntrain_raw = kel_train + man_train\neval_raw  = kel_test + man_test\n\n# Shuffle again (optional)\nrandom.shuffle(train_raw)\nrandom.shuffle(eval_raw)\n\nprint(\"Train size:\", len(train_raw))\nprint(\"Eval size:\", len(eval_raw))\nprint(\"Kelantanese test:\", len(kel_test))\nprint(\"Manglish test:\", len(man_test))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T18:03:47.619877Z","iopub.execute_input":"2026-01-02T18:03:47.620145Z","iopub.status.idle":"2026-01-02T18:03:47.628311Z","shell.execute_reply.started":"2026-01-02T18:03:47.620128Z","shell.execute_reply":"2026-01-02T18:03:47.627689Z"}},"outputs":[{"name":"stdout","text":"Train size: 1603\nEval size: 283\nKelantanese test: 283\nManglish test: 0\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"from collections import Counter\n\nCounter([x[\"dialect\"] for x in data])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T18:04:30.808743Z","iopub.execute_input":"2026-01-02T18:04:30.809288Z","iopub.status.idle":"2026-01-02T18:04:30.814960Z","shell.execute_reply.started":"2026-01-02T18:04:30.809262Z","shell.execute_reply":"2026-01-02T18:04:30.814214Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Counter({'kelantanese': 1886})"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"N = min(len(kelantanese_test), len(manglish_test))\n\nkelantanese_bal = kelantanese_test[:N]\nmanglish_bal    = manglish_test[:N]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T18:02:01.354620Z","iopub.execute_input":"2026-01-02T18:02:01.355335Z","iopub.status.idle":"2026-01-02T18:02:01.358939Z","shell.execute_reply.started":"2026-01-02T18:02:01.355308Z","shell.execute_reply":"2026-01-02T18:02:01.358210Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"save_test_predictions(\n    model,\n    kelantanese_bal,\n    path=\"/kaggle/working/decoder_only_kelantanese_balanced.csv\"\n)\n\nsave_test_predictions(\n    model,\n    manglish_bal,\n    path=\"/kaggle/working/decoder_only_manglish_balanced.csv\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T18:02:06.198185Z","iopub.execute_input":"2026-01-02T18:02:06.198746Z","iopub.status.idle":"2026-01-02T18:02:06.207310Z","shell.execute_reply.started":"2026-01-02T18:02:06.198722Z","shell.execute_reply":"2026-01-02T18:02:06.206525Z"}},"outputs":[{"name":"stdout","text":"Saved predictions to /kaggle/working/decoder_only_kelantanese_balanced.csv\nSaved predictions to /kaggle/working/decoder_only_manglish_balanced.csv\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"print(\"Kelantanese BLEU:\",\n      evaluate_bleu(model, kelantanese_test))\n\nprint(\"Manglish BLEU:\",\n      evaluate_bleu(model, manglish_test))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T18:02:11.934442Z","iopub.execute_input":"2026-01-02T18:02:11.934948Z","iopub.status.idle":"2026-01-02T18:02:45.269259Z","shell.execute_reply.started":"2026-01-02T18:02:11.934924Z","shell.execute_reply":"2026-01-02T18:02:45.268297Z"}},"outputs":[{"name":"stdout","text":"Kelantanese BLEU: 5.280700194433655\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2528310613.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m print(\"Manglish BLEU:\",\n\u001b[0;32m----> 5\u001b[0;31m       evaluate_bleu(model, manglish_test))\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_47/3047528123.py\u001b[0m in \u001b[0;36mevaluate_bleu\u001b[0;34m(model, raw_data, n)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mrefs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbleu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrefs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Decoder-only BLEU:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sacrebleu/metrics/base.py\u001b[0m in \u001b[0;36mcorpus_score\u001b[0;34m(self, hypotheses, references, n_bootstrap)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mScore\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \"\"\"\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_corpus_score_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypotheses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;31m# Collect corpus stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sacrebleu/metrics/base.py\u001b[0m in \u001b[0;36m_check_corpus_score_args\u001b[0;34m(self, hyps, refs)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"`hyps` should be a sequence of strings.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m             \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Each element of `hyps` should be a string.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhyps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"],"ename":"IndexError","evalue":"list index out of range","output_type":"error"}],"execution_count":19}]}