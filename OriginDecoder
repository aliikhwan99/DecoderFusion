{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12287144,"sourceType":"datasetVersion","datasetId":7743669},{"sourceId":12315409,"sourceType":"datasetVersion","datasetId":7762636}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Kaggle Environment Setup - Clean installation with isolated dependencies\n!pip install --force-reinstall numpy==1.26.4\n!pip install transformers==4.41.0 torch==2.1.2 sentencepiece phonemizer epitran spacy==3.7.4 sacrebleu importlib_metadata==7.1.0\n!python -m spacy download -q en_core_web_sm-3.7.0 --direct\n\n# Clean up potential conflicts\n!pip uninstall -y torchao accelerate\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-29T15:45:02.636186Z","iopub.execute_input":"2025-12-29T15:45:02.637299Z","iopub.status.idle":"2025-12-29T15:45:31.917231Z","shell.execute_reply.started":"2025-12-29T15:45:02.637262Z","shell.execute_reply":"2025-12-29T15:45:31.916096Z"}},"outputs":[{"name":"stdout","text":"Collecting numpy==1.26.4\n  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\nUsing cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\nInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.37.0 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\ngradio 5.38.1 requires typer<1.0,>=0.12; sys_platform != \"emscripten\", but you have typer 0.9.4 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\ntorchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.2 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.26.4\nCollecting transformers==4.41.0\n  Using cached transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\nRequirement already satisfied: torch==2.1.2 in /usr/local/lib/python3.11/dist-packages (2.1.2)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\nRequirement already satisfied: phonemizer in /usr/local/lib/python3.11/dist-packages (3.3.0)\nRequirement already satisfied: epitran in /usr/local/lib/python3.11/dist-packages (1.34.0)\nRequirement already satisfied: spacy==3.7.4 in /usr/local/lib/python3.11/dist-packages (3.7.4)\nRequirement already satisfied: sacrebleu in /usr/local/lib/python3.11/dist-packages (2.5.1)\nRequirement already satisfied: importlib_metadata==7.1.0 in /usr/local/lib/python3.11/dist-packages (7.1.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (2.32.5)\nCollecting tokenizers<0.20,>=0.19 (from transformers==4.41.0)\n  Using cached tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (4.67.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (4.15.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (2.18.1)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (12.1.105)\nRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (2.1.0)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (1.0.13)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (2.0.11)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (3.0.10)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (8.2.5)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (2.5.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (0.9.4)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (6.4.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (2.12.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (75.2.0)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (3.5.0)\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata==7.1.0) (3.23.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2) (12.5.82)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from phonemizer) (1.5.2)\nRequirement already satisfied: segments in /usr/local/lib/python3.11/dist-packages (from phonemizer) (2.3.0)\nRequirement already satisfied: attrs>=18.1 in /usr/local/lib/python3.11/dist-packages (from phonemizer) (25.4.0)\nRequirement already satisfied: dlinfo in /usr/local/lib/python3.11/dist-packages (from phonemizer) (2.0.0)\nRequirement already satisfied: panphon>=0.20 in /usr/local/lib/python3.11/dist-packages (from epitran) (0.22.2)\nRequirement already satisfied: marisa-trie in /usr/local/lib/python3.11/dist-packages (from epitran) (1.2.1)\nRequirement already satisfied: jamo in /usr/local/lib/python3.11/dist-packages (from epitran) (0.4.1)\nRequirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (3.2.0)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (1.2.0)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.4) (1.3.0)\nRequirement already satisfied: unicodecsv in /usr/local/lib/python3.11/dist-packages (from panphon>=0.20->epitran) (0.14.1)\nRequirement already satisfied: editdistance in /usr/local/lib/python3.11/dist-packages (from panphon>=0.20->epitran) (0.8.1)\nRequirement already satisfied: munkres in /usr/local/lib/python3.11/dist-packages (from panphon>=0.20->epitran) (1.1.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from panphon>=0.20->epitran) (2.2.3)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.4) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.4) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.4) (0.4.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (2025.10.5)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.4) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.4) (0.1.5)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.10.0,>=0.3.0->spacy==3.7.4) (8.3.0)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.4.0,>=0.1.0->spacy==3.7.4) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.2) (3.0.3)\nRequirement already satisfied: csvw>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from segments->phonemizer) (3.7.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.2) (1.3.0)\nRequirement already satisfied: isodate in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (0.7.2)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (2.9.0.post0)\nRequirement already satisfied: rfc3986<2 in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (1.5.0)\nRequirement already satisfied: uritemplate>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (4.2.0)\nRequirement already satisfied: babel in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (2.17.0)\nRequirement already satisfied: language-tags in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (1.2.0)\nRequirement already satisfied: rdflib in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (7.5.0)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (3.1.0)\nRequirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (4.25.0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->panphon>=0.20->epitran) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->panphon>=0.20->epitran) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->csvw>=1.5.6->segments->phonemizer) (1.17.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (2025.4.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (0.26.0)\nRequirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib->csvw>=1.5.6->segments->phonemizer) (3.0.9)\nUsing cached transformers-4.41.0-py3-none-any.whl (9.1 MB)\nUsing cached tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\nInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.2\n    Uninstalling tokenizers-0.15.2:\n      Successfully uninstalled tokenizers-0.15.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.37.0\n    Uninstalling transformers-4.37.0:\n      Successfully uninstalled transformers-4.37.0\nSuccessfully installed tokenizers-0.19.1 transformers-4.41.0\n\n\u001b[38;5;1mâœ˜ No compatible package found for '-q' (spaCy v3.7.4)\u001b[0m\n\n\u001b[33mWARNING: Skipping torchao as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mFound existing installation: accelerate 0.26.1\nUninstalling accelerate-0.26.1:\n  Successfully uninstalled accelerate-0.26.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -q sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T15:45:31.919550Z","iopub.execute_input":"2025-12-29T15:45:31.919963Z","iopub.status.idle":"2025-12-29T15:45:35.984758Z","shell.execute_reply.started":"2025-12-29T15:45:31.919932Z","shell.execute_reply":"2025-12-29T15:45:35.983633Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!pip install -q epitran panphon\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T15:45:35.986169Z","iopub.execute_input":"2025-12-29T15:45:35.986587Z","iopub.status.idle":"2025-12-29T15:45:40.175977Z","shell.execute_reply.started":"2025-12-29T15:45:35.986543Z","shell.execute_reply":"2025-12-29T15:45:40.174475Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport time\nimport random\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\n\nfrom transformers import (\n    GPT2LMHeadModel,\n    GPT2Tokenizer,\n    get_scheduler\n)\n\nfrom sacrebleu import corpus_bleu\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MorphologyEmbedder(nn.Module):\n    def __init__(self, device=None):\n        super().__init__()\n        from transformers import BertTokenizer, BertModel\n\n        self._device = device\n        self.tokenizer = BertTokenizer.from_pretrained(\"imvladikon/charbert-bert-wiki\")\n        self.model = BertModel.from_pretrained(\"imvladikon/charbert-bert-wiki\").to(device)\n        self.char_embedding = nn.Embedding(128, 64).to(device)\n        self.embedding_dim = self.model.config.hidden_size + 64\n\n    def _get_char_embeddings(self, text):\n        chars = [ord(c) if ord(c) < 128 else 0 for c in text[:64]]\n        chars += [0] * (64 - len(chars))\n        return self.char_embedding(torch.tensor(chars, device=self._device)).mean(dim=0)\n\n    def forward(self, texts: List[str]):\n        inputs = self.tokenizer(\n            texts, return_tensors=\"pt\",\n            padding=True, truncation=True, max_length=128\n        ).to(self._device)\n\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n\n        bert_emb = outputs.last_hidden_state.mean(dim=1)\n        char_embs = torch.stack([self._get_char_embeddings(t) for t in texts])\n        return torch.cat([bert_emb, char_embs], dim=-1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset Conversion (CRITICAL CHANGE)","metadata":{}},{"cell_type":"code","source":"class KelantanesePhonemeEmbeddings(nn.Module):\n    def __init__(self, embedding_dim=256, device=None):\n        super().__init__()\n\n        self._device = device\n        self.embedding_dim = embedding_dim\n\n        # Phoneme inventory (unchanged)\n        self.phoneme_map = {\n            p: i for i, p in enumerate(\"pbtdÊˆÉ–kgÊ”mnÉ²Å‹shlrwjiÉ›aÉ™É”ou\")\n        }\n        self.pad_id = len(self.phoneme_map)\n\n        self.embedding = nn.Embedding(\n            len(self.phoneme_map) + 1, embedding_dim\n        ).to(device)\n\n        # ---- Try to load epitran safely ----\n        try:\n            import epitran\n            self.epi = epitran.Epitran(\"msa-Latn\")\n            self.use_epitran = True\n            print(\"âœ… Epitran loaded for phoneme embeddings\")\n        except Exception as e:\n            self.epi = None\n            self.use_epitran = False\n            print(\"âš ï¸ Epitran not available â€” using fallback phoneme encoding\")\n\n    def forward(self, texts: List[str]):\n        batch_embeddings = []\n\n        for t in texts:\n            if self.use_epitran:\n                phonemes = [\n                    p for p in self.epi.transliterate(t)\n                    if p in self.phoneme_map\n                ]\n            else:\n                # ---- Fallback: character-based proxy ----\n                phonemes = list(t)\n\n            phonemes = phonemes[:64]\n\n            ids = [\n                self.phoneme_map.get(p, self.pad_id)\n                for p in phonemes\n            ]\n\n            # Padding\n            if len(ids) < 64:\n                ids += [self.pad_id] * (64 - len(ids))\n\n            ids = torch.tensor(ids, device=self._device)\n\n            emb = self.embedding(ids).mean(dim=0)\n            batch_embeddings.append(emb)\n\n        return torch.stack(batch_embeddings)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training Loop (Minimal Change)","metadata":{}},{"cell_type":"code","source":"class KelantaneseDecoderOnly(nn.Module):\n    def __init__(self, model_name=\"distilgpt2\"):\n        super().__init__()\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.tokenizer.padding_side = \"left\"\n\n        self.model = GPT2LMHeadModel.from_pretrained(model_name).to(self.device)\n\n        self.morph = MorphologyEmbedder(device=self.device)\n        self.phoneme = KelantanesePhonemeEmbeddings(device=self.device)\n\n        self.fusion = nn.Sequential(\n            nn.Linear(self.morph.embedding_dim + 256, self.model.config.n_embd),\n            nn.GELU(),\n            nn.LayerNorm(self.model.config.n_embd)\n        ).to(self.device)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        texts = self.tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n\n        morph_emb = self.morph(texts)\n        phon_emb = self.phoneme(texts)\n        fused = self.fusion(torch.cat([morph_emb, phon_emb], dim=-1))\n\n        inputs_embeds = self.model.transformer.wte(input_ids)\n        inputs_embeds = inputs_embeds + fused.unsqueeze(1)\n\n        return self.model(\n            inputs_embeds=inputs_embeds,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"jsonl_path = \"/kaggle/input/finaldialectdataset/finalDialect dataset.jsonl\"\n\nall_data = []\n\nwith open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n    for i, line in enumerate(f):\n        try:\n            obj = json.loads(line.strip())\n            if isinstance(obj, dict):\n                src = obj.get(\"kelantanese\")\n                tgt = obj.get(\"stdMalay\") or obj.get(\"english\")\n                if src and tgt:\n                    all_data.append((src, tgt))\n        except Exception as e:\n            print(f\"âŒ Line {i+1} error: {e}\")\n\nprint(f\"âœ… Loaded {len(all_data)} sentence pairs\")\n\nrandom.shuffle(all_data)\nsplit = int(0.8 * len(all_data))\ntrain_data = all_data[:split]\ntest_data  = all_data[split:]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DecoderTranslationDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=128):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        src, tgt = self.data[idx]\n\n        text = f\"Source: {src}\\nTarget: {tgt}\"\n        enc = self.tokenizer(\n            text,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n\n        input_ids = enc.input_ids.squeeze()\n        attention_mask = enc.attention_mask.squeeze()\n        labels = input_ids.clone()\n\n        # mask Source part\n        src_len = len(self.tokenizer(f\"Source: {src}\\nTarget:\", add_special_tokens=False)[\"input_ids\"])\n        labels[:src_len] = -100\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels,\n            \"raw_text\": text\n        }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dummy Data","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport time\nimport random\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\n\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer, get_scheduler\nfrom sacrebleu import corpus_bleu\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\n# -----------------------------\n# Dataset Class\n# -----------------------------\nclass DecoderTranslationDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=128):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        src, tgt = self.data[idx]\n        text = f\"Source: {src}\\nTarget: {tgt}\"\n        enc = self.tokenizer(\n            text,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n        input_ids = enc.input_ids.squeeze()\n        attention_mask = enc.attention_mask.squeeze()\n        labels = input_ids.clone()\n\n        # mask Source part\n        src_len = len(self.tokenizer(f\"Source: {src}\\nTarget:\", add_special_tokens=False)[\"input_ids\"])\n        labels[:src_len] = -100\n\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels, \"raw_text\": text}\n\n# -----------------------------\n# Model Class\n# -----------------------------\nclass KelantaneseDecoderOnly(nn.Module):\n    def __init__(self, model_name=\"distilgpt2\"):\n        super().__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.tokenizer.padding_side = \"left\"  # âš ï¸ important for decoder-only\n\n        self.model = GPT2LMHeadModel.from_pretrained(model_name).to(self.device)\n\n        # Dummy morph/phoneme emb placeholders for simplicity\n        self.fusion = nn.Identity()  # replace with fusion layers if needed\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        return self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n\n# -----------------------------\n# Load Data (subset for speed)\n# -----------------------------\njsonl_path = \"/kaggle/input/finaldialectdataset/finalDialect dataset.jsonl\"\nall_data = []\nwith open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        obj = json.loads(line.strip())\n        src = obj.get(\"kelantanese\")\n        tgt = obj.get(\"stdMalay\") or obj.get(\"english\")\n        if src and tgt:\n            all_data.append((src, tgt))\n\nrandom.shuffle(all_data)\n\n# Use subset for faster run\ntrain_data = all_data[:512]\ntest_data = all_data[512:640]\n\n# -----------------------------\n# DataLoader\n# -----------------------------\nmodel = KelantaneseDecoderOnly()\ntokenizer = model.tokenizer\ndevice = model.device\n\ntrain_dataset = DecoderTranslationDataset(train_data, tokenizer)\ntest_dataset  = DecoderTranslationDataset(test_data, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader  = DataLoader(test_dataset, batch_size=16)\n\n# -----------------------------\n# Optimizer + Scheduler\n# -----------------------------\noptimizer = AdamW(model.parameters(), lr=5e-4)\nepochs = 5\nsteps = len(train_loader) * epochs\n\nscheduler = get_scheduler(\n    \"linear\",\n    optimizer,\n    num_warmup_steps=int(0.1 * steps),\n    num_training_steps=steps\n)\n\n# -----------------------------\n# Training Loop\n# -----------------------------\ntraining_losses, bleu_scores, epoch_times = [], [], []\n\nprint(f\"\\nðŸš€ Training on {device}\")\n\nfor epoch in range(epochs):\n    start_time = time.time()\n    total_loss = 0\n    model.train()\n\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        batch_inputs = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\", \"labels\"]}\n        outputs = model(**batch_inputs)\n        loss = outputs.loss\n\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_loader)\n    training_losses.append(avg_loss)\n\n    # -----------------------------\n    # BLEU evaluation\n    # -----------------------------\n    model.eval()\n    preds, refs = [], []\n\n    with torch.no_grad():\n        for batch in test_loader:\n            inputs = batch[\"input_ids\"].to(device)\n            masks  = batch[\"attention_mask\"].to(device)\n\n            gen = model.model.generate(\n                input_ids=inputs,\n                attention_mask=masks,\n                max_new_tokens=40,\n                pad_token_id=tokenizer.eos_token_id\n            )\n\n            decoded = tokenizer.batch_decode(gen, skip_special_tokens=True)\n            for d, r in zip(decoded, batch[\"raw_text\"]):\n                if \"Target:\" in d and \"Target:\" in r:\n                    preds.append(d.split(\"Target:\")[-1].strip())\n                    refs.append([r.split(\"Target:\")[-1].strip()])\n\n    bleu = corpus_bleu(preds, refs).score if preds else 0.0\n    bleu_scores.append(bleu)\n    epoch_time = time.time() - start_time\n    epoch_times.append(epoch_time)\n\n    print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | BLEU: {bleu:.2f} | Time: {epoch_time:.1f}s\")\n\n# -----------------------------\n# Visualization\n# -----------------------------\nplt.figure(figsize=(15,5))\nplt.subplot(1,3,1)\nplt.plot(training_losses, label=\"Loss\")\nplt.title(\"Training Loss\")\nplt.grid(True)\n\nplt.subplot(1,3,2)\nplt.plot(bleu_scores, label=\"BLEU\", color=\"green\")\nplt.title(\"BLEU Score\")\nplt.grid(True)\n\nplt.subplot(1,3,3)\nplt.bar(range(1, epochs+1), epoch_times, color=\"skyblue\")\nplt.title(\"Epoch Duration (s)\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# -----------------------------\n# Sample Predictions\n# -----------------------------\nmodel.eval()\nfor src, tgt in random.sample(test_data, min(5, len(test_data))):\n    prompt = f\"Source: {src}\\nTarget:\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    gen = model.model.generate(\n        **inputs,\n        max_new_tokens=40,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    out = tokenizer.decode(gen[0], skip_special_tokens=True)\n    print(\"SOURCE:\", src)\n    print(\"PRED  :\", out.split(\"Target:\")[-1].strip())\n    print(\"GOLD  :\", tgt)\n    print(\"-\"*60)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# VERSION 2 -Correction","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n\nimport json\nimport time\nimport random\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\n\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer, get_scheduler\nfrom sacrebleu import corpus_bleu\nfrom tqdm import tqdm\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MorphologyEmbedder(nn.Module):\n    def __init__(self, device=None):\n        super().__init__()\n        from transformers import BertTokenizer, BertModel\n\n        self._device = device\n        self.tokenizer = BertTokenizer.from_pretrained(\"imvladikon/charbert-bert-wiki\")\n        self.model = BertModel.from_pretrained(\"imvladikon/charbert-bert-wiki\").to(device)\n        self.char_embedding = nn.Embedding(128, 64).to(device)\n        self.embedding_dim = self.model.config.hidden_size + 64\n\n    def _get_char_embeddings(self, text):\n        chars = [ord(c) if ord(c) < 128 else 0 for c in text[:64]]\n        chars += [0] * (64 - len(chars))\n        return self.char_embedding(torch.tensor(chars, device=self._device)).mean(dim=0)\n\n    def forward(self, texts: List[str]):\n        inputs = self.tokenizer(\n            texts, return_tensors=\"pt\",\n            padding=True, truncation=True, max_length=128\n        ).to(self._device)\n\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n\n        bert_emb = outputs.last_hidden_state.mean(dim=1)\n        char_embs = torch.stack([self._get_char_embeddings(t) for t in texts])\n        return torch.cat([bert_emb, char_embs], dim=-1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class KelantanesePhonemeEmbeddings(nn.Module):\n    def __init__(self, embedding_dim=256, device=None):\n        super().__init__()\n        self._device = device\n        self.embedding_dim = embedding_dim\n\n        self.phoneme_map = {p: i for i, p in enumerate(\"pbtdÊˆÉ–kgÊ”mnÉ²Å‹shlrwjiÉ›aÉ™É”ou\")}\n        self.pad_id = len(self.phoneme_map)\n\n        self.embedding = nn.Embedding(len(self.phoneme_map)+1, embedding_dim).to(device)\n\n        try:\n            import epitran\n            self.epi = epitran.Epitran(\"msa-Latn\")\n            self.use_epitran = True\n        except:\n            self.use_epitran = False\n\n    def forward(self, texts: List[str]):\n        batch = []\n\n        for t in texts:\n            phonemes = (\n                [p for p in self.epi.transliterate(t) if p in self.phoneme_map]\n                if self.use_epitran else list(t)\n            )[:64]\n\n            ids = [self.phoneme_map.get(p, self.pad_id) for p in phonemes]\n            ids += [self.pad_id] * (64 - len(ids))\n\n            ids = torch.tensor(ids, device=self._device)\n            batch.append(self.embedding(ids).mean(dim=0))\n\n        return torch.stack(batch)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class KelantaneseDecoderOnly(nn.Module):\n    def __init__(self, model_name=\"distilgpt2\"):\n        super().__init__()\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.tokenizer.padding_side = \"left\"\n\n        self.gpt = GPT2LMHeadModel.from_pretrained(model_name).to(self.device)\n\n        self.morph = MorphologyEmbedder(device=self.device)\n        self.phoneme = KelantanesePhonemeEmbeddings(device=self.device)\n\n        self.fusion = nn.Sequential(\n            nn.Linear(self.morph.embedding_dim + self.phoneme.embedding_dim,\n                      self.gpt.config.n_embd),\n            nn.GELU(),\n            nn.LayerNorm(self.gpt.config.n_embd)\n        ).to(self.device)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        texts = self.tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n\n        morph_emb = self.morph(texts)\n        phon_emb = self.phoneme(texts)\n        fused = self.fusion(torch.cat([morph_emb, phon_emb], dim=-1))\n\n        token_embs = self.gpt.transformer.wte(input_ids)\n        token_embs = token_embs + fused.unsqueeze(1)\n\n        return self.gpt(\n            inputs_embeds=token_embs,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n\n    # ðŸ”‘ THIS IS THE FIX\n    def generate_with_fusion(self, prompts, max_new_tokens=40):\n        enc = self.tokenizer(prompts, return_tensors=\"pt\", padding=True).to(self.device)\n\n        texts = prompts\n        morph_emb = self.morph(texts)\n        phon_emb = self.phoneme(texts)\n        fused = self.fusion(torch.cat([morph_emb, phon_emb], dim=-1))\n\n        token_embs = self.gpt.transformer.wte(enc.input_ids)\n        token_embs = token_embs + fused.unsqueeze(1)\n\n        return self.gpt.generate(\n            inputs_embeds=token_embs,\n            attention_mask=enc.attention_mask,\n            max_new_tokens=max_new_tokens,\n            pad_token_id=self.tokenizer.eos_token_id\n        )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DecoderTranslationDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=128):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        src, tgt = self.data[idx]\n        prompt = f\"Source: {src}\\nTarget:\"\n        full = f\"{prompt} {tgt}\"\n\n        enc = self.tokenizer(\n            full, truncation=True, padding=\"max_length\",\n            max_length=self.max_length, return_tensors=\"pt\"\n        )\n\n        input_ids = enc.input_ids.squeeze()\n        attention_mask = enc.attention_mask.squeeze()\n        labels = input_ids.clone()\n\n        prompt_len = len(self.tokenizer(prompt, add_special_tokens=False)[\"input_ids\"])\n        labels[:prompt_len] = -100\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels,\n            \"prompt\": prompt,\n            \"target\": tgt\n        }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = KelantaneseDecoderOnly()\ntokenizer = model.tokenizer\ndevice = model.device\n\ntrain_loader = DataLoader(train_data, batch_size=4, shuffle=True)\ntest_loader  = DataLoader(test_data, batch_size=4)\n\noptimizer = AdamW(model.parameters(), lr=5e-4)\nepochs = 3\n\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        batch = {k: v.to(device) for k, v in batch.items() if k not in [\"prompt\", \"target\"]}\n        loss = model(**batch).loss\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        total_loss += loss.item()\n\n    # ---- BLEU ----\n    model.eval()\n    preds, refs = [], []\n\n    with torch.no_grad():\n        for batch in test_loader:\n            gen = model.generate_with_fusion(batch[\"prompt\"])\n            decoded = tokenizer.batch_decode(gen, skip_special_tokens=True)\n\n            for d, tgt in zip(decoded, batch[\"target\"]):\n                preds.append(d.split(\"Target:\")[-1].strip())\n                refs.append([tgt])\n\n    bleu = corpus_bleu(preds, refs).score\n    print(f\"Epoch {epoch+1} | Loss {total_loss/len(train_loader):.4f} | BLEU {bleu:.2f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GPT-ONLY","metadata":{}},{"cell_type":"code","source":"# =========================\n# C1: Environment\n# =========================\nimport os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n\n# =========================\n# C2: Imports\n# =========================\nimport json\nimport time\nimport random\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\n\nfrom transformers import (\n    GPT2LMHeadModel,\n    GPT2Tokenizer,\n    get_scheduler\n)\n\nfrom sacrebleu import corpus_bleu\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\n# =========================\n# C3: Decoder-only Dataset\n# =========================\nclass DecoderTranslationDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=128):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        src, tgt = self.data[idx]\n\n        prompt = f\"Source: {src}\\nTarget:\"\n        full_text = f\"{prompt} {tgt}\"\n\n        enc = self.tokenizer(\n            full_text,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n\n        input_ids = enc.input_ids.squeeze()\n        attention_mask = enc.attention_mask.squeeze()\n        labels = input_ids.clone()\n\n        # ðŸ”‘ mask source tokens\n        prompt_len = len(\n            self.tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n        )\n        labels[:prompt_len] = -100\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels,\n            \"target\": tgt\n        }\n\n# =========================\n# C4: Pure Decoder-only GPT\n# =========================\nclass KelantaneseDecoderOnly(nn.Module):\n    def __init__(self, model_name=\"distilgpt2\"):\n        super().__init__()\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.tokenizer.padding_side = \"left\"  # REQUIRED\n\n        self.model = GPT2LMHeadModel.from_pretrained(model_name).to(self.device)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        return self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n\n# =========================\n# C5: Load Dataset\n# =========================\njsonl_path = \"/kaggle/input/finaldialectdataset/finalDialect dataset.jsonl\"\n\nall_data = []\nwith open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        obj = json.loads(line.strip())\n        src = obj.get(\"kelantanese\")\n        tgt = obj.get(\"stdMalay\") or obj.get(\"english\")\n        if src and tgt:\n            all_data.append((src, tgt))\n\nrandom.shuffle(all_data)\n\n# ðŸ”½ limit size for speed (optional)\ntrain_data = all_data[:512]\ntest_data  = all_data[512:640]\n\nprint(f\"Train: {len(train_data)} | Test: {len(test_data)}\")\n\n# =========================\n# C6: Dataloader\n# =========================\nmodel = KelantaneseDecoderOnly()\ntokenizer = model.tokenizer\ndevice = model.device\n\ntrain_loader = DataLoader(\n    DecoderTranslationDataset(train_data, tokenizer),\n    batch_size=4,\n    shuffle=True\n)\n\ntest_loader = DataLoader(\n    DecoderTranslationDataset(test_data, tokenizer),\n    batch_size=8\n)\n\n# =========================\n# C7: Optimizer & Scheduler\n# =========================\noptimizer = AdamW(model.parameters(), lr=5e-4)\nepochs = 5\nsteps = len(train_loader) * epochs\n\nscheduler = get_scheduler(\n    \"linear\",\n    optimizer,\n    num_warmup_steps=int(0.1 * steps),\n    num_training_steps=steps\n)\n\n# =========================\n# C8: Training + BLEU\n# =========================\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    start = time.time()\n\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        batch = {k: v.to(device) for k, v in batch.items() if k != \"target\"}\n\n        out = model(**batch)\n        loss = out.loss\n\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_loader)\n\n    # ---------- BLEU ----------\n    model.eval()\n    preds, refs = [], []\n\n    with torch.no_grad():\n        for batch in test_loader:\n            inputs = batch[\"input_ids\"].to(device)\n            masks  = batch[\"attention_mask\"].to(device)\n\n            gen = model.model.generate(\n                input_ids=inputs,\n                attention_mask=masks,\n                max_new_tokens=40,\n                pad_token_id=tokenizer.eos_token_id\n            )\n\n            decoded = tokenizer.batch_decode(gen, skip_special_tokens=True)\n\n            for d, tgt in zip(decoded, batch[\"target\"]):\n                if \"Target:\" in d:\n                    preds.append(d.split(\"Target:\")[-1].strip())\n                    refs.append([tgt])\n\n    bleu = corpus_bleu(preds, refs).score if preds else 0.0\n\n    print(\n        f\"Epoch {epoch+1} | \"\n        f\"Loss {avg_loss:.4f} | \"\n        f\"BLEU {bleu:.2f} | \"\n        f\"Time {time.time()-start:.1f}s\"\n    )\n\n# =========================\n# C9: Sample Predictions\n# =========================\nmodel.eval()\nfor src, tgt in random.sample(test_data, min(5, len(test_data))):\n    prompt = f\"Source: {src}\\nTarget:\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n    gen = model.model.generate(\n        **inputs,\n        max_new_tokens=40,\n        pad_token_id=tokenizer.eos_token_id\n    )\n\n    out = tokenizer.decode(gen[0], skip_special_tokens=True)\n\n    print(\"SOURCE:\", src)\n    print(\"PRED  :\", out.split(\"Target:\")[-1].strip())\n    print(\"GOLD  :\", tgt)\n    print(\"-\" * 60)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ---VERSION 2 DECODER ONLY","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade accelerate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T15:45:40.178591Z","iopub.execute_input":"2025-12-29T15:45:40.179031Z","iopub.status.idle":"2025-12-29T15:45:45.031811Z","shell.execute_reply.started":"2025-12-29T15:45:40.178999Z","shell.execute_reply":"2025-12-29T15:45:45.030501Z"}},"outputs":[{"name":"stdout","text":"Collecting accelerate\n  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.1.3)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.3)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.36.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.10.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.18.1)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\nRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.1.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate) (12.5.82)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.10.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\nDownloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\nSuccessfully installed accelerate-1.12.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Restart Kaggle kernel first\n!pip uninstall -y transformers peft\n!pip install transformers==4.37.0  # stable version\n!pip install datasets sacrebleu torch\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q peft==0.7.1 accelerate==0.25.0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T15:45:45.033366Z","iopub.execute_input":"2025-12-29T15:45:45.034087Z","iopub.status.idle":"2025-12-29T15:45:49.617771Z","shell.execute_reply.started":"2025-12-29T15:45:45.034044Z","shell.execute_reply":"2025-12-29T15:45:49.616457Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!pip install -q \\\n  transformers==4.37.0 \\\n  accelerate==0.26.1 \\\n  peft==0.7.1 \\\n  datasets sacrebleu\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T15:45:49.619127Z","iopub.execute_input":"2025-12-29T15:45:49.619435Z","iopub.status.idle":"2025-12-29T15:46:02.239406Z","shell.execute_reply.started":"2025-12-29T15:45:49.619408Z","shell.execute_reply":"2025-12-29T15:46:02.238397Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.37.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# =========================================================\n# 0ï¸âƒ£ Environment\n# =========================================================\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# =========================================================\n# 1ï¸âƒ£ Imports\n# =========================================================\nimport json\nimport random\nimport glob\nimport torch\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    Trainer,\n    TrainingArguments\n)\nfrom sacrebleu.metrics import CHRF\n\n# =========================================================\n# 2ï¸âƒ£ Locate Dataset Files\n# =========================================================\n\nKELANTAN_DIR = \"/kaggle/input/finaldialectdataset\"\nMANGLISH_DIR = \"/kaggle/input/filtered-manglish-1-8kv2\"\n\ndef find_jsonl_files(folder):\n    files = glob.glob(os.path.join(folder, \"*.jsonl\"))\n    if not files:\n        raise FileNotFoundError(f\"No .jsonl files found in {folder}\")\n    return files\n\nkelantan_files = find_jsonl_files(KELANTAN_DIR)\nmanglish_files = find_jsonl_files(MANGLISH_DIR)\n\n# =========================================================\n# 3ï¸âƒ£ Load Data\n# =========================================================\n\ndef load_kelantanese(files):\n    data = []\n    for path in files:\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                row = json.loads(line)\n                if \"kelantanese\" in row and \"stdMalay\" in row:\n                    data.append({\n                        \"dialect\": \"kelantanese\",\n                        \"source\": row[\"kelantanese\"],\n                        \"target\": row[\"stdMalay\"]\n                    })\n    return data\n\ndef load_manglish(files):\n    data = []\n    for path in files:\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                row = json.loads(line)\n                if \"text\" in row and \"malay\" in row:\n                    data.append({\n                        \"dialect\": \"manglish\",\n                        \"source\": row[\"text\"],\n                        \"target\": row[\"malay\"]\n                    })\n    return data\n\ndata = load_kelantanese(kelantan_files) + load_manglish(manglish_files)\nrandom.shuffle(data)\n\nprint(f\"Total samples: {len(data)}\")\n\nassert len(data) > 0, \"Dataset is EMPTY â€” check your files!\"\n\n# =========================================================\n# 4ï¸âƒ£ Train / Test Split\n# =========================================================\n\nsplit = int(0.85 * len(data))\ntrain_ds = Dataset.from_list(data[:split])\ntest_ds  = Dataset.from_list(data[split:])\n\nprint(f\"Train: {len(train_ds)} | Test: {len(test_ds)}\")\n\n# =========================================================\n# 5ï¸âƒ£ Tokenizer + GPT-2\n# =========================================================\n\nMODEL_NAME = \"gpt2\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\nmodel.resize_token_embeddings(len(tokenizer))\n\nMAX_LEN = 256\n\n# =========================================================\n# 6ï¸âƒ£ Preprocessing\n# =========================================================\n\ndef preprocess(batch):\n    input_ids, labels, attention_mask = [], [], []\n\n    for d, s, t in zip(batch[\"dialect\"], batch[\"source\"], batch[\"target\"]):\n        prompt = (\n            f\"<|dialect|> {d}\\n\"\n            f\"<|source|> {s}\\n\"\n            f\"<|target|> \"\n        )\n\n        full = prompt + t + tokenizer.eos_token\n        enc = tokenizer(\n            full,\n            max_length=MAX_LEN,\n            padding=\"max_length\",\n            truncation=True\n        )\n\n        label = enc[\"input_ids\"].copy()\n        prompt_len = len(tokenizer(prompt)[\"input_ids\"])\n\n        for i in range(prompt_len):\n            label[i] = -100\n\n        input_ids.append(enc[\"input_ids\"])\n        labels.append(label)\n        attention_mask.append(enc[\"attention_mask\"])\n\n    return {\n        \"input_ids\": input_ids,\n        \"labels\": labels,\n        \"attention_mask\": attention_mask\n    }\n\ntrain_ds = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\ntest_ds  = test_ds.map(preprocess, batched=True, remove_columns=test_ds.column_names)\n\ntrain_ds.set_format(\"torch\")\ntest_ds.set_format(\"torch\")\n\n# =========================================================\n# 7ï¸âƒ£ Metrics\n# =========================================================\n\nchrf = CHRF()\n\ndef compute_metrics(eval_pred):\n    preds, labels = eval_pred\n\n    preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    labels = tokenizer.batch_decode(\n        [[l for l in lab if l != -100] for lab in labels],\n        skip_special_tokens=True\n    )\n\n    preds = [p.split(\"<|target|>\")[-1].strip() for p in preds]\n\n    return {\n        \"chrF\": chrf.corpus_score(preds, [labels]).score\n    }\n\n# =========================================================\n# 8ï¸âƒ£ Training Arguments\n# =========================================================\n\nargs = TrainingArguments(\n    output_dir=\"./gpt_dialect\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=3e-5,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=8,\n    num_train_epochs=10,\n    fp16=torch.cuda.is_available(),\n    logging_steps=50,\n    save_total_limit=2,\n    report_to=\"none\"\n)\n\n# =========================================================\n# 9ï¸âƒ£ Trainer\n# =========================================================\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_ds,\n    eval_dataset=test_ds,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n\n# =========================================================\n# ðŸ”Ÿ Inference\n# =========================================================\n\ndef translate(text, dialect):\n    prompt = (\n        f\"<|dialect|> {dialect}\\n\"\n        f\"<|source|> {text}\\n\"\n        f\"<|target|> \"\n    )\n    ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n    out = model.generate(ids, max_new_tokens=80)\n    return tokenizer.decode(out[0], skip_special_tokens=True).split(\"<|target|>\")[-1]\n\nprint(\"Kelantanese â†’ Malay:\")\nprint(translate(\"Nasi ni sedho sikit\", \"kelantanese\"))\n\nprint(\"\\nManglish â†’ Malay:\")\nprint(translate(\"aiyo dont do like that lah\", \"manglish\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T15:33:58.399652Z","iopub.execute_input":"2025-12-29T15:33:58.400083Z","iopub.status.idle":"2025-12-29T15:34:09.018317Z","shell.execute_reply.started":"2025-12-29T15:33:58.400059Z","shell.execute_reply":"2025-12-29T15:34:09.015946Z"}},"outputs":[{"name":"stderr","text":"2025-12-29 15:34:04.527940: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767022444.554048     183 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767022444.562084     183 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_183/2084885431.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_kelantanese\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkelantan_files\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mload_manglish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmanglish_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_183/2084885431.py\u001b[0m in \u001b[0;36mload_manglish\u001b[0;34m(files)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"text\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"malay\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                     data.append({\n","\u001b[0;32m/usr/lib/python3.11/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 1 column 129 (char 128)"],"ename":"JSONDecodeError","evalue":"Expecting ',' delimiter: line 1 column 129 (char 128)","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# -----------","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass DecoderTranslationDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=64):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        src, tgt = self.data[idx]\n\n        prompt = f\"Source: {src}\\nTarget: \"\n        full_text = prompt + tgt\n\n        enc = self.tokenizer(\n            full_text,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        input_ids = enc[\"input_ids\"].squeeze(0)\n        attention_mask = enc[\"attention_mask\"].squeeze(0)\n\n        labels = input_ids.clone()\n\n        # Mask prompt part\n        prompt_ids = self.tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].squeeze(0)\n        labels[:len(prompt_ids)] = -100\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels,\n            \"raw_text\": full_text\n        }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\nclass KelantaneseDecoderOnly(nn.Module):\n    def __init__(self, model_name=\"distilgpt2\"):\n        super().__init__()\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.tokenizer.padding_side = \"left\"\n\n        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n        self.model.resize_token_embeddings(len(self.tokenizer))\n        self.model.to(self.device)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        return self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nfrom torch.utils.data import DataLoader\nfrom transformers import AdamW, get_scheduler\nfrom tqdm import tqdm\nfrom sacrebleu import corpus_bleu\n\nmodel = KelantaneseDecoderOnly()\ntokenizer = model.tokenizer\ndevice = model.device\n\ntrain_dataset = DecoderTranslationDataset(train_data, tokenizer)\ntest_dataset  = DecoderTranslationDataset(test_data, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\ntest_loader  = DataLoader(test_dataset, batch_size=8)\n\noptimizer = AdamW(model.parameters(), lr=5e-4)\nepochs = 3\n\nsteps = len(train_loader) * epochs\nscheduler = get_scheduler(\n    \"linear\",\n    optimizer,\n    num_warmup_steps=int(0.1 * steps),\n    num_training_steps=steps\n)\n\nprint(f\"\\nðŸš€ Training on {device}\")\n\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    start = time.time()\n\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        batch = {k: v.to(device) for k, v in batch.items() if k != \"raw_text\"}\n\n        out = model(**batch)\n        loss = out.loss\n\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_loader)\n\n    # ---------- Evaluation ----------\n    model.eval()\n    preds, refs = [], []\n\n    with torch.no_grad():\n        for batch in test_loader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n\n            gen = model.model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_new_tokens=20,\n                min_new_tokens=2,     # ðŸ”¥ CRITICAL FIX\n                do_sample=False,\n                pad_token_id=tokenizer.eos_token_id\n            )\n\n            decoded = tokenizer.batch_decode(gen, skip_special_tokens=True)\n\n            for d, r in zip(decoded, batch[\"raw_text\"]):\n                if \"Target:\" in d and \"Target:\" in r:\n                    preds.append(d.split(\"Target:\")[-1].strip())\n                    refs.append([r.split(\"Target:\")[-1].strip()])\n\n    bleu = corpus_bleu(preds, refs).score if preds else 0.0\n    epoch_time = time.time() - start\n\n    print(\n        f\"Epoch {epoch+1} | \"\n        f\"Loss {avg_loss:.4f} | \"\n        f\"BLEU {bleu:.2f} | \"\n        f\"Time {epoch_time:.1f}s\"\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}